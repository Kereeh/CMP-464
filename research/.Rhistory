library(dplyr)
rt %>% filter(place_name == 'California')
summary(rt$place_name)
rt$place_name
rt %>% filter(place_name = "California") %>% count()
rt %>% filter(place_name == "California") %>% count()
rt <- search_tweets(
q = query, n = n,
since = since, geocode = geocode
)
geocode <- "36.78, -119,42"
rt <- search_tweets(
q = query, n = n,
since = since, geocode = geocode
)
rt <-
search_tweets(
q = query,
since = since,
geocode = geocode
)
query <- "fire"
since <- "2018-11-14"
geocode <- "36.78, -119,42"
rt <-
search_tweets(
q = query,
since = since,
geocode = geocode
)
q = query,
rt <-
search_tweets(
q = query,
since = since,
geocode = lookup_coords("california")
)
rt <-
search_tweets(
q = query,
since = since
)
query <- "fire"
since <- "2018-11-14"
geocode <- "36.78, -119,42"
rt <-
search_tweets(
q = query,
since = since
)
rt <-
search_tweets(
q = query,
since = since,
unclude_rts = FALSE
)
View(rt)
query <- "california fire"
since <- "2018-11-14"
geocode <- "36.78, -119,42"
rt <-
search_tweets(
q = query,
since = since,
unclude_rts = FALSE
)
View(rt)
rt %>%
s
rt %>% select(user_id, created_at, text, is_quote, is_retweet, retweet_count, hashtags, place_name, protected, followers_count, friends_count, verified)
query <- "california fire"
since <- "2018-11-14"
rt <-
search_tweets(
q = query,
since = since,
include_rts = FALSE
)
rt <-
rt %>%
select(user_id, created_at, text, is_quote, is_retweet,
retweet_count, hashtags, place_name, protected,
followers_count, friends_count, verified)
View(rt)
query <- "california fire"
since <- "2018-11-14"
rt <-
search_tweets(
q = query,
n = 1000,
since = since,
include_rts = FALSE
)
rt <-
rt %>%
select(user_id, created_at, text, is_quote, is_retweet,
retweet_count, hashtags, place_name, protected,
followers_count, friends_count, verified)
View(rt)
rt <-
search_tweets(
q = query,
n = 1000,
since = since,
until = until
include_rts = FALSE
query <- "california fire"
since <- "2018-11-14"
until <- "2018-11-15"
rt <-
search_tweets(
q = query,
n = 1000,
since = since,
until = until,
include_rts = FALSE
)
rt <-
rt %>%
select(user_id, created_at, text, is_quote, is_retweet,
retweet_count, hashtags, place_name, protected,
followers_count, friends_count, verified)
View(rt)
query <- "california fire"
since <- "2018-11-08"
until <- "2018-11-09"
rt <-
search_tweets(
q = query,
n = 1000,
since = since,
until = until,
include_rts = FALSE
)
rt <-
rt %>%
select(user_id, created_at, text, is_quote, is_retweet,
retweet_count, hashtags, place_name, protected,
followers_count, friends_count, verified)
View(rt)
write.csv(rt, file = "tweets_1108_1109.csv")
query <- "california fire"
since <- "2018-11-08"
until <- "2018-11-09"
rt <-
search_tweets(
q = query,
n = 1000,
since = since,
until = until,
include_rts = FALSE
)
rt <-
rt %>%
select(user_id, created_at, text, is_quote, is_retweet,
retweet_count, hashtags, place_name, protected,
followers_count, friends_count, verified)
write.csv(rt, file = "tweets_1108_1109.csv")
rt <- rt %>% mutate(hashtags = as.character(hashtags))
write.csv(rt, file = "tweets_1108_1109.csv")
query <- "california fire"
since <- "2018-11-08"
until <- "2018-11-09"
chico <- lookup_coords("california", "country:US")
rt <-
search_tweets(
q = query,
n = 10000,
geocode = chico,
since = since,
until = until,
include_rts = FALSE
)
rt <-
rt %>%
select(user_id, created_at, text, is_quote, is_retweet,
retweet_count, hashtags, place_name, protected,
followers_count, friends_count, verified)
View(chico)
chico
chico <- lookup_coords("chico, CA", "country:US")
chico <- "39.73, -121.84"
rt <-
search_tweets(
q = query,
n = 10000,
geocode = chico,
since = since,
until = until,
include_rts = FALSE
)
rt <-
rt %>%
select(user_id, created_at, text, is_quote, is_retweet,
retweet_count, hashtags, place_name, protected,
followers_count, friends_count, verified)
install.packages("rtweet")
library(rtweet)
## web browser method: create token and save it as an environment variable
create_token(
app = "get_fire_tweets",
consumer_key = "GCtW4oGByWTKM63Fl9AfT0yCd",
consumer_secret = "ZqjitWlzjqQPE4mJN8HBZ1lCZO3fQumGs03ibDTi60rfXo5uab
")
query <- "california fire"
since <- "2018-11-08"
until <- "2018-11-09"
rt <-
search_tweets(
q = query,
n = 10000,
since = since,
until = until,
include_rts = FALSE
)
rt <-
rt %>%
select(user_id, created_at, text, is_quote, is_retweet,
retweet_count, hashtags, place_name, protected,
followers_count, friends_count, verified)
library(dplyr)
rt <-
rt %>%
select(user_id, created_at, text, is_quote, is_retweet,
retweet_count, hashtags, place_name, protected,
followers_count, friends_count, verified)
rt <- rt %>% mutate(hashtags = as.character(hashtags))
View(rt)
write.csv(rt, file = "tweets_1108_1109.csv")
\
anova(chickwts)
anova(lm(chickwts))
head(chickwts)
library(dplyr)
library(e1071)
library(gridExtra)
library(ggplot2)
theme_set(theme_bw())
#reading in the data
df <- read.csv("vgsales.csv")
#since df contains 16k observations, lets limit the data to one year
df <-
df %>%
filter(Year == '2016')
#don't want to visualize games with 0 sales so let's remove that
df <-
df %>%
filter(NA_Sales > 0)
#Calculating the means of the sales
na_mean <- mean(df$NA_Sales)
eu_mean <- mean(df$EU_Sales)
jp_mean <- mean(df$JP_Sales)
#Calculating the standard deviations of the sales
na_sd <- sd(df$NA_Sales)
eu_sd <- sd(df$EU_Sales)
jp_sd <- sd(df$JP_Sales)
#saving the data into a dataframe
region <- c('North America', 'Europe', 'Japan')
mean <- c(na_mean, eu_mean, jp_mean)
standard_deviation <- c(na_sd, eu_sd, jp_sd)
df1 <- data.frame(region, mean, standard_deviation)
png(filename = "mean_sd.png")
grid.table(df1)
dev.off()
df %>%
ggplot(aes(x = NA_Sales)) +
geom_histogram(color = "black", fill="#56B4E9") +
xlab('Sales in North America (in millions)') +
ylab('Number of Games') +
labs(title = 'Distribution of game sales in North America')
df %>%
ggplot(aes(x = NA_Sales)) +
geom_histogram(color = "black", fill="#56B4E9") +
scale_x_log10() +
xlab('Sales in North America (in millions)') +
ylab('Number of Games') +
labs(title = 'Distribution of game sales in North America')
df %>%
ggplot(aes(x = reorder(Platform, -table(Platform)[Platform]))) +
geom_bar(aes(color = "black", fill=Platform)) +
guides(fill=FALSE) +
xlab('Video Game Platforms') +
ylab('Frequency') +
labs(title = 'Distribution of Video Game Platforms')
df %>%
ggplot(aes(x = reorder(Platform, -table(Platform)[Platform]))) +
geom_bar(aes(fill=Platform), color = "black") +
guides(fill=FALSE) +
xlab('Video Game Platforms') +
ylab('Frequency') +
labs(title = 'Distribution of Video Game Platforms')
na_plot <-
df %>%
ggplot(aes(x = NA_Sales)) +
geom_histogram(color = "black", fill="#56B4E9") +
scale_x_log10() +
xlab('Sales in North America (in millions)') +
ylab('Number of Games') +
labs(title = 'Distribution of game sales in North America')
eu_plot <-
df %>%
ggplot(aes(x = EU_Sales)) +
geom_histogram(color = "black", fill="#009E73") +
scale_x_log10() +
xlab('Sales in Europe (in millions)') +
ylab('Number of Games') +
labs(title = 'Distribution of game sales in Europe')
jp_plot <-
df %>%
ggplot(aes(x = JP_Sales)) +
geom_histogram(color = "black", fill="#D55E00") +
scale_x_log10() +
xlab('Sales in Japan (in millions)') +
ylab('Number of Games') +
labs(title = 'Distribution of game sales in Japan')
View(na_plot)
#Platform
platform_distribution <-
df %>%
ggplot(aes(x = reorder(Platform, -table(Platform)[Platform]))) +
geom_bar(aes(fill=Platform), color = "black") +
guides(fill=FALSE) +
xlab('Video Game Platforms') +
ylab('Frequency') +
labs(title = 'Distribution of Video Game Platforms')
genre_distribution <-
df %>%
ggplot(aes(x = reorder(Genre, -table(Genre)[Genre]), fill=Genre)) +
guides(fill=FALSE) +
geom_bar(color = "black") +
xlab('Video Game Genre') +
ylab('Frequency') +
labs(title = 'Distribution of Video Game Genre')
publisher_distribution <-
df %>%
ggplot(aes(x = reorder(Publisher, table(Publisher)[Publisher]))) +
geom_bar(aes(fill=Publisher), color = "black") +
guides(fill=FALSE) +
xlab('Video Game Publisher') +
ylab('Frequency') +
labs(title = 'Distribution of Video Game Publisher') +
coord_flip()
platform_distribution
genre_distribution
publisher_distribution
InsectSprays
anova(InsectSprays)
head(InsectSprays)
anova(lm(count ~ spray, data = InsectSprays))
boxplot(InsectSprays)
aov
TukeyHSD(aov(count ~ spray, data = InsectSprays))
setwd("C:/Users/kerim/GitHub/cmp_indstudy_ml/research")
library(tidyverse)
dsh_2011 <- read_csv('dsh_2011.csv')
states <- read.csv('expansion_states.csv')
library(tidyverse)
dsh_2011 <- read_csv('dsh_2011.csv')
dsh_2013 <- read_csv('dsh_2013.csv')
dsh_2015 <- read_csv('dsh_2015.csv')
states <- read.csv('expansion_states.csv')
```{r, include = FALSE}
library(tidyverse)
dsh_2011 <- read_csv('dsh_2011.csv')
dsh_2013 <- read_csv('dsh_2013.csv')
dsh_2015 <- read_csv('dsh_2015.csv')
states <- read.csv('expansion_states.csv')
dsh_2011 <-
dsh_2011 %>%
select(state, dsh1, dsh2, dsh3) %>%
group_by(state) %>%
sum(dsh1, dsh2, dsh3)
dsh_2011 <-
dsh_2011 %>%
select(state, dsh1, dsh2, dsh3) %>%
group_by(state) %>%
sum(dsh_2011$dsh1, dsh_2011$dsh2, dsh_2011$dsh3)
dsh_2011 <-
dsh_2011 %>%
select(state, dsh1, dsh2, dsh3)
View(dsh_2011)
dsh_2011 <-
dsh_2011 %>%
aggregate(dsh1, by = state, FUN=sum)
dsh_2011 <-
dsh_2011 %>%
aggregate(dsh1, by = sh_2011$state, FUN=sum)
dsh_2011 <-
dsh_2011 %>%
aggregate(dsh1, by = dsh_2011$state, FUN=sum)
dsh_2011 <-
dsh_2011 %>%
aggregate(dsh1, by = list(dsh_2011$state), FUN=sum)
dsh_2011 <-
aggregate(dsh_2011$dsh1, by = list(dsh_2011$state), FUN=sum)
View(dsh_2011)
library(tidyverse)
dsh_2011 <- read_csv('dsh_2011.csv')
dsh_2013 <- read_csv('dsh_2013.csv')
dsh_2015 <- read_csv('dsh_2015.csv')
states <- read.csv('expansion_states.csv')
dsh_2011 <-
dsh_2011 %>%
select(state, dsh1, dsh2, dsh3)
dsh_2011 <-
dsh_2011 %>%
group_by(state) %>%
summarize_all(sum)
View(dsh_2011)
library(tidyverse)
dsh_2011 <- read_csv('dsh_2011.csv')
dsh_2013 <- read_csv('dsh_2013.csv')
dsh_2015 <- read_csv('dsh_2015.csv')
states <- read.csv('expansion_states.csv')
dsh_2011[is.na(dsh_2011)] <- 0
dsh_2013[is.na(dsh_2013)] <- 0
dsh_2015[is.na(dsh_2015)] <- 0
dsh_2011 <-
dsh_2011 %>%
select(state, dsh1, dsh2, dsh3)
dsh_2011 <-
dsh_2011 %>%
group_by(state) %>%
summarize_all(sum)
View(dsh_2011)
library(tidyverse)
dsh_2011 <- read_csv('dsh_2011.csv')
dsh_2013 <- read_csv('dsh_2013.csv')
dsh_2015 <- read_csv('dsh_2015.csv')
states <- read.csv('expansion_states.csv')
dsh_2011[is.na(dsh_2011)] <- 0
dsh_2013[is.na(dsh_2013)] <- 0
dsh_2015[is.na(dsh_2015)] <- 0
dsh_2011 <-
dsh_2011 %>%
select(state, dsh1, dsh2, dsh3)
dsh_2011 <-
dsh_2011 %>%
group_by(state) %>%
summarize_all(sum)
dsh_2013 <-
dsh_2013 %>%
select(state, dsh1, dsh2, dsh3)
dsh_2013 <-
dsh_2013 %>%
group_by(state) %>%
summarize_all(sum)
dsh_2015 <-
dsh_2015 %>%
select(state, dsh1, dsh2, dsh3)
dsh_2015 <-
dsh_2015 %>%
group_by(state) %>%
summarize_all(sum)
dsh_2015$dsh3 <- as.numeric(dsh_2015$dsh3)
dsh_2015 <-
dsh_2015 %>%
group_by(state) %>%
summarize_all(sum)
View(dsh_2011)
View(dsh_2013)
View(dsh_2015)
dsh_2011$dsh11 <-
dsh_2011$dsh1 + dsh_2011$dsh2 + dsh_2011 + dsh3
dsh_2011$dsh11 <-
dsh_2011$dsh1 + dsh_2011$dsh2 + dsh_2011$dsh3
dsh_2011 <-
dsh_2011 %>%
select(state, dsh11)
dsh_2013$dsh13 <-
dsh_2013$dsh1 + dsh_2013$dsh2 + dsh_2013$dsh3
dsh_2013 <-
dsh_2013 %>%
select(state, dsh13)
dsh_2015$dsh15 <-
dsh_2015$dsh1 + dsh_2015$dsh2 + dsh_2015$dsh3
dsh_2015 <-
dsh_2015 %>%
select(state, dsh15)
dsh <- merge(dsh_2011, dsh_2015, by = 'state)
''
''
dsh <- merge(dsh_2011, dsh_2015, by = 'state')
dsh <- merge(dsh, dsh_2013, by = 'state')
head(dsh)
dsh$15-11 <-
dsh$15to11 <-
dsh$x15to11 <-
(dsh$dsh15 - dsh$dsh11)/(dsh$dsh11)
dsh$x13to11 <-
(dsh$dsh13 - dsh$dsh11)/(dsh$dsh11)
states
dsh_states <- merge(dsh, states, by = 'state')
View(dsh_states)
model <- glm(expansion_status ~ x15to11, data = dsh_states, family = 'binomial')
summary(model)
curve(predict(model, data.frame(x15to11 = x), type = "resp"), add = TRUE)
plot(x15to11, expansion_status, data = dsh_states)
plot(dsh_states$x15to11, dsh_states$expansion_status)
curve(predict(model, data.frame(x15to11 = x), type = "resp"))
model <- glm(expansion_status ~ x15to11, data = dsh_states, family = 'binomial')
newdat <- data.frame(dsh_states$x15to11 = seq(min(dsh_states$x15to11), max = dsh_states$x15to11), len = 100)
model <- glm(expansion_status ~ x15to11, data = dsh_states, family = 'binomial')
newdat <- data.frame(x15to11 = seq(min(dsh_states$x15to11), max = dsh_states$x15to11), len = 100)
newdat$expansion_status <- predict(model, newdata = newdat, type = "response")
plot(expansion_status~x15to11, data = dsh_states, col = "red4")
lines(expansion_status~x15to11, newdat, col="green4", lwd=2)
model <- glm(expansion_status ~ x15to11, data = dsh_states, family = 'binomial')
xweight <- data.frame(x15to11 = seq(min(dsh_states$x15to11), max = dsh_states$x15to11), len = 100)
yweight <- predict(model, list(expansion_status = xweight), type = "response")
yweight <- predict(model, list(dsh_states$expansion_status = xweight), type = "response")
yweight <- predict(model, list(expansion_status = xweight), type = "response")
